\section{Scheduler}

\paragraph{Fairness.}
The only requirement for a scheduler for reactive components is fairness.
Fairness, as applied to reactive components, means that every action receives an infinite number of opportunities to execute.

\paragraph{Single-threaded and multi-threaded designs.}
A fair single-threaded scheduler is straight-forward to implement by cycling through all the actions in a round-robin fashion.
Minimizing scheduling overhead by not giving opportunities to disable actions is the main optimization.
The same principles apply to multi-threaded schedulers with added challenge of avoiding data races.
That is, a multi-threaded scheduler cannot simultaneous execute two actions that may mutate the same state.
A multi-threaded scheduler must either prevent the various threads from selecting conflicting actions or implement a protocol that allows the threads to negotiate the conflict.
A scheduler might prevent conflicts by computing a schedule for each thread with explicit synchronization points.
The advantage of this approach is that fairness is easily enforced.
The drawbacks of this approach is that it is very rigid and may lead to excessive overhead and idle time waiting for synchronization.
Conflict can be negotiated by either selecting a different action (deferral) or blocking until there is no longer a conflict.
To enforce fairness, an action cannot be deferred indefinitely.
The remainder of the discussion focuses on the design and implementation of a multi-threaded scheduler.

\paragraph{Steady-state assumption.}
To make the design of the system tractable, we assume that the system achieves a steady-state for all executions.
For infinite executions, this typically involves the use of flow control/feedback to prevent producers of work from overrunning consumers of work.
For finite executions, the system must always achieve a \emph{fixed-point} which is a state where every action is disabled.
Suppose a producer task P executes on one thread and produces one work item for a consumer task C executing on another thread.
Furthermore, suppose that P is always enabled and is executed proportionally faster than C.
An infinite execution is fair, i.e., it will have and infinite number of P's and C's, however, C will never ``catch up'' to P.
If memory is allocated for every item produced by P, then the system will require an unbounded amount of memory.
In general, if a producer task that generates $n$ work items for a consumer task, then the consumer task must be executed $n$ times faster to ``keep up.''
The steady-state assumption means that the relative frequency of action executions is irrelevant for correctness.
We believe the assumption of a steady-state is not overly restrictive as all real-world systems have this property.

\paragraph{Motivation for a partitioned scheduler.}
A straightforward approach to constructing a multi-threaded scheduler is to place all actions on a global list and have each thread execute the next action on the list.
This organization has three potential problems.
First, the global list is a shared resource.
As the number of processors increases, so will contention on the list.
Second, locking is mandatory to avoid data races.
To execute an action, a thread must acquire a lock for all instances involved in the action.
Third, this approach requires extra logic for good cache behavior.
A thread may not want the next action on the list if the action has no affinity for the thread.

In a partitioned scheduler, the actions are partitioned among the available threads.
This organization eliminates contention over a shared global list and has the potential for good cache behavior.
Data races can be avoided through partitioning and locking.
