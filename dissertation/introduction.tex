\chapter{Introduction}
\label{introduction}

\section{Reactive Programs and Systems}

Manna and Pnueli classify programs as either being \emph{transformational} or \emph{reactive}~\cite{manna1992temporal}.
As implied by their name, transformational programs transform a finite input sequence into a finite output sequence.
Transformational programs often can be divided into three distinct phases corresponding to the activities of input, processing, and output.
A compiler is an example of a transformational program as it transforms a source file into an object file.
Formal models of computation like the Turing machine~\cite{turing1936computable} and $\lambda$-calculus~\cite{church1936unsolvable} are concerned with transformational programs.

In contrast, a reactive program is characterized by ``ongoing interactions with its environment''~\cite{manna1992temporal}.
A reactive program must share resources with its environment to facilitate communication.
From the perspective of the reactive program, input occurs when the environment acts on the reactive program and output occurs when the reactive program acts on the environment.
The manipulation of resources internal to a reactive program or an environment is generically referred to as processing.

Whereas transformational programs are designed to halt and produce an output, reactive programs are often designed to run forever.
The activities of input, processing, and output are thus recurring and often overlapped in the execution of a reactive program.
Designers of reactive programs then must often reason about infinite event sequences containing interleaved input, processing, and output.
A web server is a example of a reactive program as it repeatedly receives requests, processes them, and sends responses.
Reactive systems include operating systems, databases, networked applications, interactive applications, and embedded systems.
A number of formal models have been developed to reason about reactive systems and include the Calculus of Communicating Systems~\cite{milner1982calculus}, the Algebra of Communicating Processes~\cite{bergstra1982fixed}, Cooperating Sequential Processes~\cite{dijkstra1965cooperating}, Communicating Sequential Processes~\cite{hoare1978communicating}, Kahn process networks~\cite{kahn1974semantics}, the Actor Model~\cite{hewitt1973universal}\cite{clinger1981foundations}\cite{agha1985actors}, UNITY~\cite{chandy1989parallel}, and I/O automata~\cite{nancy1996distributed}.

Asynchronous concurrency is a fundamental characteristic of reactive systems that makes them difficult to design and develop.
Concurrency refers to the idea that a reactive program and its environment may act at the same time.
In synchronous models, a reactive program and its environment share a common clock which allows the reactive program to coordinate access to shared resources, e.g., the environment writes a shared variable in one clock cycle and the reactive program reads it in the next.
However, shared clocks are difficult to implement and do not scale.
Consequently, many reactive systems are asynchronous meaning that the reactive program and environment evolve independently with respect to time.
To facilitate communication, asynchronous models include facilities for atomicity that allow a reactive program to act without being interrupted by its environment and vice versa.
These facilities for atomicity allow a reactive program to synchronize with its environment, as the atomic events play the role of a shared clock.
A common approach to ensuring correctness in asynchronous models is to view the environment as an adversary that will deliver inputs at inopportune times.

State is fundamental to reactive systems and may be modeled directly or indirectly.
Some examples of indirect approaches to modeling state include monads~\cite{wadler1990comprehending} which are typically used in functional languages such as Haskell, or mail queues with message-behavior pairs~\cite{agha1985actors} which are used in actor-oriented languages such as Erlang.
The imperative programming paradigm models state directly using variables and assignment and the dominant languages used to implement reactive systems, such as C, C++, and Java, are based on this paradigm.
As we are interested in expressing reactive semantics directly, we limit the remaining discussion to reactive systems based on the imperative programming paradigm.

The imperative programming paradigm when presented using structured programming techniques attempts to express a (transformational) computation as a sequence of statements.
The computation being performed is realized by executing each statement in the sequence where a statement is either an assignment statement, a condition (if/else), or a loop.
Control flows from one statement to the next unless altered by a condition or loop.
A locus of control is called a \emph{thread}.
Statements compose sequentially, that is, a sequence of statements can be thought of as a single statement that performs the computation of the sequence in one step.
Imperative programming languages often permit the definition of procedures, e.g., subroutines, functions, macros, etc., as a way to abstract sequences of statements or expressions.
Control passes from the calling sequence to the sequence indicated by the procedure and returns to the calling sequence when the procedure terminates.
The semantics of calling a procedure are completely compatible with the sequential composition of statements.

Many reactive programs are multi-threaded and designed to use the facilities of a conventional operating system.
For the discussion to follow, we consider each thread in a multi-threaded program to be a reactive program.
The environment for a thread then is the operating system and the other threads with which it directly interacts.
This matches the definition of a reactive program as the thread will have ongoing interactions with its environment, i.e., interact with the operating system and other threads.
This also matches the definition of asynchronous concurrency as the thread and its environment share no common clock and can interact asynchronously.
To be more specific, a system call is asynchronous from the perspective of the operating system and threads may receive asynchronous signals from the operating system and each other.

\section{Trends}

Developments in hardware and software platforms have resulted in an increasing demand for reactive systems.
Embedded systems continue to proliferate due to advances in hardware that continue to produce new platforms, form factors, sensors, actuators, and price points, which allow embedded computers to be applied to a variety of application domains.
Individuals, businesses, and governments are also deploying networks of sensors and actuators to monitor, control, and coordinate critical infrastructures such as power grids and telecommunication networks.
These advances also have led to platforms for individual users, such as smart phones, e-readers, and tablets.
Applications for these personal platforms are necessarily interactive and therefore reactive.
The leveling-off of processor speeds and the resulting trend toward multicore processors is also creating demand for reactive systems, as increases in performance must come through increasingly concurrent applications~\cite{sutter2005software}.

A general trend toward distribution is also driving demand for reactive systems.
Increasingly, network services form the core (or are at least a critical component) of many applications and are fundamental to delivering the content (e.g., downloading books and movies) and communication (e.g., social media) that drive the application.
More and more devices are being equipped with (especially wireless) network adapters due to the introduction of inexpensive networking technologies.
Networks are now also \emph{emerging}, as opposed to being intentionally deployed, in environments such as the home, office, hospitals, etc.
Applications that take advantage of these new networks are necessarily reactive.
The trend toward distribution is already established in enterprise computing infrastructures where networked systems like file servers, print servers, web servers, application servers, and databases are critical or central to supporting business processes and achieving business objectives.

Given the continued proliferation of reactive systems, their number, diversity, and complexity is likely to increase as they encompass more and more interactions.
This is most evident in large-scale distributed systems where a computation is spread over a variety of nodes.
Such systems often \emph{evolve} as new sub-systems are introduced and integrated into the existing infrastructure.
The individual nodes themselves may also contain a variety of interactions.
For example, it is not uncommon to find a smart phone application that simultaneously interacts with the user via a graphical user interface, an application server via an Internet connection, and sensors (e.g., accelerometers) that are embedded in the hardware.
Similarly, sophisticated servers like web servers and databases are often built from collections of interacting reactive modules.

\section{Limitations of the State of the Art}

Failure to account for asynchronous concurrency may result in a reactive program with \emph{timing bugs}, meaning that correct execution depends on arbitrary scheduling decisions.
Timing bugs may be manifested when the schedule of events is perturbed, e.g., due to the introduction of a new processor or operating system, or when part of the program takes more or less time than normal.
Operating systems are particularly susceptible to timing bugs caused by device drivers~\cite{ryzhyk2009dingo}.
Timing bugs can escape even a rigorous software development process and may lay dormant for years~\cite{lee2006problem}.
Furthermore, timing bugs are notoriously difficult to diagnose, inspiring the term \emph{heisenbug}, a bug that disappears or changes its behavior when someone is attempting to find it (because the debugging process alters the timing of the program)~\cite{1983proceedings}.
Timing bugs can cost many hours of debugging time and can lead to a poor user experience, e.g., a non-responsive device or application, and loss of revenue, e.g., when an advertisement service cannot be used while a server reboots.

Introducing asynchronous concurrency into the imperative programming paradigm places a burden on developers as they must explicitly identify the statement sequences that must atomic.
The misuse of synchronization primitives, which becomes likely as state transitions become complex, may introduce concurrency hazards (e.g., deadlock~\cite{dijkstra1965cooperating}) which manifest themselves as timing bugs.
A number of design patterns for concurrency have been developed to help developers avoid concurrency hazards~\cite{schmidt2000pattern}, \cite{lea2000concurrent}.
These design patterns represent a move toward implicit atomicity as they often attempt to leverage language features to control atomicity (e.g., scoped locking~\cite{schmidt2000pattern}).
While some of these patterns have been incorporated into programming languages (e.g., the \verb+synchronized+ keyword of Java implements the thread-safe interface pattern~\cite{schmidt2000pattern}), they are most often enforced only by convention and therefore easily violated or ignored (e.g., if a new developer is unaware of the convention).
In practice, explicit atomicity and the corresponding use of synchronization primitives has proven to be tedious and error prone~\cite{sutter2005software}.

Correct synchronization in sequential imperative programs is a holistic problem that resists encapsulation.
Sequential imperative programs, both transformational and reactive, are often designed using functionally modular principles such as procedural programming, object-oriented programming, and functional programming.
A transition in a reactive program then is typically distributed over a variety of modules, i.e., the graph of procedure calls.
The challenge for a developer then is to identify all of the shared state and then use synchronization primitives to guard concurrent updates.
Proper synchronization is based upon a complete understanding of the call graph (which may not be fully known due to aliasing).
The resulting code tends to be brittle as modifications tend to introduce new timing bugs.
%% Existing approaches to synchronization attempt to impose structure on the call graph, e.g., an object can only participate in one transition at a time (thread-safe interface~\cite{schmidt2000pattern}), to avoid concurrency hazards.
%% The most common formal approach to verifying correct synchronization is model checking which has its own limitation, e.g., the size and fidelity of the model.
%% The active object pattern~\cite{schmidt2000pattern} is an attempt to encapsulate the behavior of a reactive program.

\section{Challenges}
\label{challenges}

\paragraph{Reduce accidental complexity.}
A key challenge towards adequately supporting complex reactive systems is to reduce the accidental complexity associated with their design and implementation.
For software, accidental complexity is defined as the ``difficulties that today attend its production but that are not inherent~\cite{brooks1995mythical}.''
One source of accidental complexity is the \emph{conflation of semantics}, where a problem naturally expressed using one set of semantics is implemented with a different set of semantics resulting in a semantic gap and obfuscation.
To illustrate, Lee shows how the common practice of introducing thread-based concurrency via a library to an inherently sequential language significantly alters the semantics of the language~\cite{lee2006problem}.
As described in the previous section, we claim that the currently dominant approaches to developing reactive systems rely on inherently transformational languages that have been augmented with features for concurrency, which introduces an example of the kind of problem that Lee has identified.
Thus, reducing the accidental complexity in reactive systems requires an approach that provides direct support for reactive semantics and addresses the inherent difficulties of asynchronous concurrency.

\paragraph{Achieve principled composition and decomposition.}
\emph{Decomposition} and \emph{composition} are essential techniques when designing, implementing, and understanding complex systems.
Decomposition, dividing a complex system into a number of simpler systems, is often used when \emph{designing} a system, e.g., through top-down design~\cite{wirth1971program}.
Composition, building a complex system from a number of simpler systems, is often used when \emph{implementing} a system; i.e., simpler systems are implemented, tested, and integrated to create larger systems~\cite{brooks1995mythical}.
Often, the simplest systems in a design are common and can be reused across problem domains.
Similarly, systems in the same problem domain often have common sub-systems.
Thus, a common goal in software engineering is fostering design processes that produce and leverage reusable components.
Decomposition also often imparts a logical organization to a system when the resulting sub-systems are cohesive, i.e., each sub-system has a well-defined purpose~\cite{parnas1972criteria}.
Thus, decomposition is a significant aid to understanding and managing complex systems.

Asynchronous concurrency undermines decomposition and composition when not properly encapsulated and therefore limits our ability to design and implement complex reactive systems.
Such problems of asynchronous concurrency stem from the interactions between reactive programs.
Decomposition increases the number of reactive programs constituting a system which in turn increases the number of susceptible interactions and opportunities for timing bugs.
For decomposition to achieve an overall reduction in complexity when designing a reactive system, it must reduce the amount of reasoning that must be performed at each level of the design.
Thus, it must be possible to replace the details of how a reactive program is implemented with higher-level statements about its behavior in terms of its interface.

A second challenge then is to ensure that reactive systems can be decomposed and composed in a principled way.
A design process based on composition and decomposition tends to be effective when the model adheres to certain principles:
\begin{enumerate}
\item The model should define units of composition and a means of composition.
Obviously, a model that does not define a unit of composition and a means of composition cannot support a design process based on composition or decomposition.

\item The result of composition should either be a well-formed entity in the model or be undefined.
Thus, it is impossible to create an entity whose behavior and properties go beyond the scope of the model and therefore cannot be understood in terms of the model.
When the result of composition is defined, it should often (if not always) be a unit of composition.
This principle facilitates reuse and permits decomposition to an arbitrary \emph{degree}.

\item A unit of composition should be able to encapsulate other units of composition.
When this principle is combined with the previous principle, the result is \emph{recursive encapsulation} which permits decomposition to an arbitrary \emph{depth}.
Recursive encapsulation allows the system being designed to take on a hierarchical organization.

\item The behavior of a unit of composition should be encapsulated by its interface.
Encapsulation allows one to hide implementation details and is necessary for abstraction.

%% \item The model, unit of composition, and composition operator(s) should be defined so that composition can only reduce the set of possible behaviors that can be exhibited by a unit of composition.
%% That is, given a unit of composition with behavior set $B$ and an instance of composition resulting in a new behavior set $B'$, then it must be the case that $B' \subseteq B$.
%% Given that properties are assertions over behavior, this principle can be restated to say that composition can only restrict properties proved in isolation.
%% This principle facilitates reuse since the possible behaviors of a unit of composition is fixed.

\item Composition should be \emph{compositional} meaning that the properties of a unit of composition can be stated in terms of the properties of its constituent units of composition.
Thus, when attempting to understand an entity resulting from composition, one need only examine its constituent parts and their interactions.
To illustrate, consider a system $X$ that is a pipeline formed by composing a filter system $F$ with reliable FIFO channel system $C$.
This principle states that the properties of the composed Filter-Channel $X$ can be expressed in terms of the properties of the Filter $F$ and Channel $C$.
%% The more interesting question is if the properties of $X$ can be systematically derived from $F$ and $C$ as this facilitates hierarchical reasoning.

\item Units of composition should have some notion of substitutional equivalence.
If a unit of composition $X$ contains a unit of composition $Y$, then a unit of composition $X'$ formed by substituting the definition of $Y$ into $X$ should be equivalent to $X$.
Substitutional equivalence guarantees that we can compose and decompose at will and summarizes the preceding principles.
We believe that substitution should be linear in the size of the units of composition.
To illustrate, suppose that $X$ and $Y$ are mathematical functions in the description above. %% and substitution, therefore, is equivalent to substituting function definitions.
%% Thus, if we take the size of a unit of composition to be the terms in a function, then $|X'| \approx |X| + |Y|$.
If we take the size of a unit of composition to be the number of terms in the definition of a function then $|X'| \approx |X| + |Y|$.
\end{enumerate}
A model adhering to these principles facilitates and supports \emph{principled composition and decomposition}.
Principled composition requires language support for interfaces, definitions, and substitutional equivalence.
A variety of useful domains including mathematical expressions, object-oriented programming, functional programming, and digital logic circuits support principled composition.

Reactive programs based on threads are not subject to principled decomposition and composition.
To illustrate, consider three imperative reactive programs (threads) $A$, $X$, and $Y$ where $A$ is composed of $X$ and $Y$.
Principled composition requires that the definition of $A$ can be formed by substituting the definitions of $X$ and $Y$.
To preserve the reactive semantics when composing $X$ and $Y$, one must consider all pairs of transitions, i.e., the Cartesian product, which represents all possible interleavings between the two statement sequences.
The result of composition then is a two-dimensional torus where each node represents a compound state, each edge represents a transition, and each direction corresponds to executing a statement in $X$ and/or $Y$.
Appropriate measures must be taken to ensure that all transitions, i.e., all vertical, horizontal, and diagonal moves in the torus, are well-defined, i.e., explicit atomicity.
We observe that 1)~no existing platforms support the direct definition of such tori and 2)~reasoning about a two-dimensional torus (or $N$-dimensional for $N$ composed sequences) is qualitatively different that reasoning about a single sequence.
Thus, reactive programs based on the imperative programming paradigm are not subject to recursive encapsulation and substitutional equivalence.

\section{Approach and Contributions}

To reduce the accidental complexity associated with the design and implementation of reactive systems while supporting principled composition and decomposition, we propose a transactional model for reactive systems called reactive components.
A reactive component is a set of state variables and a set of atomic transitions that operate on those state variables.
Linking a transition in one component to a transition in another component yields another transition that operates on the state variables of the constituent components.
A transaction-oriented approach resolves the main difficulties of the control-oriented imperative approach.
In the reactive component model, transitions are atomic.
This relieves developers from the burden of identifying and guarding critical sections.
The composability of transitions allows developers to create complex state transitions independent of control flow.
This relieves developers from needing a holistic understanding of the call graph when composing a complex state transition.

This research makes the following contributions to the state of the art in reactive system development.
After presenting necessary background and related work in Chapter~\ref{background}, we present a novel reactive component model in Chapter~\ref{model}.
In Chapter~\ref{language}, we present \rcgo{}:  a programming language for reactive components based on the Go programming language.
For tractability, we assume systems with a fixed number of components in a fixed configuration.
Chapter~\ref{implementation} describes the implementation of an interpreter for \rcgo{} including the algorithm that checks a system for sound composition, a calling convention for transitions, the implementation of move semantics, and an approach to file descriptor I/O.
Chapter~\ref{implementation} also describes the design, implementation, and an evaluation of two concurrent weakly fair schedulers.
The schedulers are compared to a custom multi-threaded application for two reactive systems.
For one system, the reactive component implementation outperforms the custom application while the custom application outperforms the reactive component implementation for the other system.
The results demonstrate that reactive components may be a viable alternative to threads but additional work is necessary to generalize this claim.
Chapter~\ref{conclusion} presents conclusion and describes future work that is motivated and enabled by this dissertation.
