\chapter{Introduction}
\label{introduction}

\paragraph{Definitions.}
Manna and Pnueli classify programs as either being \emph{transformational} or \emph{reactive}~\cite{manna1992temporal}.
As implied by their name, transformational programs transform a finite input sequence into a finite output sequence.
Transformational programs often can be divided into three distinct phases corresponding to the activities of input, processing, and output.
A compiler is an example of a transformational program as it transforms a source file into an object file.
Formal models of computation like the Turing machine~\cite{turing1936computable} and $\lambda$-calculus~\cite{church1936unsolvable} are concerned with transformational programs.

In contrast, a reactive program is characterized by ``ongoing interactions with its environment\cite{manna1992temporal}.''
Whereas transformational programs are designed to halt and produce an output (when possible, i.e., the halting problem), reactive programs are often designed to run forever.
The activities of input, processing, and output are thus overlapped in the execution of a reactive program.
A web server is a example of a reactive program as it continually receives requests, processes them, and sends responses.
Reactive systems include operating systems, databases, networked applications, interactive applications, and embedded systems.
A number of formal models have been developed to reason about reactive systems and include the Calculus of Communicating Systems~\cite{milner1982calculus}, the Algebra of Communicating Processes~\cite{bergstra1982fixed}, Cooperating Sequential Processes~\cite{dijkstra1965cooperating}, Communicating Sequential Processes~\cite{hoare1978communicating}, Kahn process networks~\cite{kahn1974semantics}, the Actor Model~\cite{hewitt1973universal}\cite{clinger1981foundations}\cite{agha1985actors}, UNITY~\cite{chandy1989parallel}, and I/O automata~\cite{nancy1996distributed}.
A reactive program must share resources with its environment to facilitate communication.
From the perspective of the reactive program, input occurs when the environment acts on the reactive program and output occurs when the reactive program acts on the environment.
The manipulation of resources internal to a reactive program or an environment is generically referred to as processing.

Asynchronous concurrency is a fundamental characteristic of reactive systems that makes them difficult to design and develop.
Concurrency refers to the idea that a reactive program and its environment may act at the same time.
In synchronous models, a reactive program and its environment share a common clock which allows the reactive program to coordinate access to shared resources, e.g., the environment writes a shared variable in one clock cycle and the reactive program reads it in the next.
However, shared clocks are difficult to implement and do not scale.
Consequently, many reactive systems are asynchronous meaning that the reactive program and environment evolve independently with respect to time.
To facilitate communication, asynchronous models include facilities for atomicity that allow a reactive program to act without being interrupted by its environment and vice versa.
The facilities for atomicity allow a reactive program to synchronize with its environment as the atomic events play the role of a shared clock.
A common approach to ensuring correctness in asynchronous models is to view the environment as an adversary that will deliver inputs at inopportune times.
Since a reactive program may run forever, the designers of reactive programs thus must often reason about infinite event sequences containing interleaved input, output, and processing.

Failure to account for asynchronous concurrency may result in a reactive program with \emph{timing bugs}, meaning that correct execution depends on arbitrary scheduling decisions.
Timing bugs may be manifested when the schedule of events is perturbed, e.g., due to a new processor or operating system, or when part of the program takes more or less time than normal.
Operating systems are particularly susceptible to timing bugs caused by device drivers~\cite{ryzhyk2009dingo}.
Timing bugs can escape even a rigorous software development process and lay dormant for years~\cite{lee2006problem}.
Furthermore, timing bugs are notoriously difficult to diagnose, inspiring the term \emph{heisenbug}, a bug that disappears or changes its behavior when someone is attempting to find it (because the debugging process alters the timing of the program)~\cite{1983proceedings}.
Timing bugs can cost many hours of debugging time and can lead to a poor user experience, e.g., a non-responsive device or application, and loss of revenue, e.g., when an advertisement service cannot be used while a server reboots.

\emph{Decomposition} and \emph{composition} are essential techniques when designing, implementing, and understanding complex systems.
Decomposition, dividing a complex system into a number of simpler systems, is often used when \emph{designing} a system, e.g., through top-down design~\cite{wirth1971program}.
Composition, building a complex system from a number of simpler systems, is often used when \emph{implementing} a system; i.e., simpler systems are implemented, tested, and integrated to create larger systems~\cite{brooks1995mythical}.
Often, the simplest systems in a design are common and can be reused across problem domains.
Similarly, systems in the same problem domain often have common sub-systems.
Thus, a common goal in software engineering is fostering design processes that produce and leverage reusable components.
Decomposition also often imparts a logical organization to a system when the resulting sub-systems are cohesive, i.e., each sub-system has a well-defined purpose~\cite{parnas1972criteria}.
Thus, decomposition is a significant aid to understanding and managing complex systems.

\paragraph{Trends.}
Developments in hardware and software platforms have resulted in an increasing demand for reactive systems.
Embedded systems continue to proliferate due to advances in hardware that continue to produce new platforms, form factors, sensors, actuators, and price points, which allow embedded computers to be applied to a variety of application domains.
Individuals, businesses, and governments are deploying networks of sensors and actuators to monitor, control, and coordinate critical infrastructures such as power grids and telecommunication networks.
These advances also have led to platforms for individual users, such as smart phones, e-readers, and tablets.
Applications for these personal platforms are necessarily interactive and therefore reactive.
The leveling-off of processor speeds and the resulting trend toward multicore processors is also creating demand for reactive systems, as increases in performance must come through increasingly concurrent applications~\cite{sutter2005software}.

A general trend toward distribution is also driving demand for reactive systems.
Increasingly, network services form the core (or are at least a critical component) of many applications and are fundamental to delivering the content (e.g., downloading books and movies) and communication (e.g., social media) that drive the application.
More and more devices are being equipped with (especially wireless) network adapters due to the introduction of inexpensive networking technologies.
Networks are now also \emph{emerging}, as opposed to being intentionally deployed, in environments such as the home, office, hospitals, etc.
Applications that take advantage of these new networks are necessarily reactive.
The trend toward distribution is already established in enterprise computing infrastructures where networked systems like file servers, print servers, web servers, application servers, and databases are critical or central to supporting business processes and achieving business objectives.

Given the continued proliferation of reactive systems, their number, diversity, and complexity is likely to increase as they encompass more and more interactions.
This is most evident in large-scale distributed systems where a computation is spread over a variety of nodes.
Such systems often \emph{evolve} as new sub-systems are introduced and integrated into the existing infrastructure.
The individual nodes themselves may also contain a variety of interactions.
For example, it is not uncommon to find a smart phone application that simultaneously interacts with the user via a graphical user interface, an application server via an Internet connection, and sensors (e.g., accelerometers) that are embedded in the hardware.
Similarly, sophisticated servers like web servers and databases are often built from collections of interacting reactive modules.

\paragraph{Challenge:  Reduce accidental complexity.}
A key challenge towards adequately supporting such complex reactive systems is to reduce the accidental complexity associated with their design and implementation.
For software, accidental complexity is defined as the ``difficulties that today attend its production but that are not inherent~\cite{brooks1995mythical}.''
One source of accidental complexity is the \emph{conflation} of semantics, where a problem naturally expressed using one set of semantics is implemented with a different set of semantics resulting in a semantic gap and obfuscation.
To illustrate, Lee shows how the common practice of introducing thread-based concurrency via a library to an inherently sequential language significantly alters the semantics of the language~\cite{lee2006problem}.
We claim that the currently dominant approaches to developing reactive systems rely on inherently transformational languages that have been augmented with features for concurrency, which introduces an example of the kind of problem that Lee has identified.
Thus, reducing the accidental complexity in reactive systems requires a platform that provides direct support for reactive semantics.

\paragraph{Challenge:  Achieve principled composition and decomposition.}
The second challenge is to ensure that reactive systems can be decomposed and composed in a principled way.
A design process based on composition and decomposition tends to be effective when the model adheres to certain principles:
\begin{enumerate}
\item The model should define units of composition and a means of composition.
Obviously, a model that does not define a unit of composition and a means of composition cannot support a design process based on composition or decomposition.

\item The result of composition should either be a well-formed entity definable in the model or be undefined.
Thus, it is impossible to create an entity whose behavior and properties go beyond the scope of the model and therefore cannot be understood in terms of the model.
When the result of composition is defined, it should often (if not always) be a unit of composition.
This principle facilitates reuse and permits decomposition to an arbitrary \emph{degree}.

\item A unit of composition should be able to encapsulate other units of composition.
When this principle is combined with the previous principle, the result is \emph{recursive encapsulation} which permits decomposition to an arbitrary \emph{depth}.
Recursive encapsulation allows the system being designed to take on a hierarchical organization.

\item The behavior of a unit of composition should be encapsulated by its interface.
Encapsulation allows one to hide implementation details and is necessary for abstraction.

%% \item The model, unit of composition, and composition operator(s) should be defined so that composition can only reduce the set of possible behaviors that can be exhibited by a unit of composition.
%% That is, given a unit of composition with behavior set $B$ and an instance of composition resulting in a new behavior set $B'$, then it must be the case that $B' \subseteq B$.
%% Given that properties are assertions over behavior, this principle can be restated to say that composition can only restrict properties proved in isolation.
%% This principle facilitates reuse since the possible behaviors of a unit of composition is fixed.

\item Composition should be \emph{compositional} meaning that the properties of a unit of composition can be stated in terms of the properties of its constituent units of composition.
Thus, when attempting to understand an entity resulting from composition, one need only examine its constituent parts and their interactions.
To illustrate, consider a system $X$ that is a pipeline formed by composing a filter system $F$ with reliable FIFO channel system $C$.
This principle states that the properties of the composed Filter-Channel $X$ can be expressed in terms of the properties of the Filter $F$ and Channel $C$.
%% The more interesting question is if the properties of $X$ can be systematically derived from $F$ and $C$ as this facilitates hierarchical reasoning.

\item Units of composition should have some notion of substitutional equivalence.
If a unit of composition $X$ contains a unit of composition $Y$, then a unit of composition $X'$ formed by substituting the definition of $Y$ into $X$ should be equivalent to $X$.
Substitutional equivalence guarantees that we can compose and decompose at will and summarizes the preceding principles.
We believe that substitution should be linear in the size of the units of composition.
To illustrate, suppose that $X$ and $Y$ are mathematical functions in the description above. %% and substitution, therefore, is equivalent to substituting function definitions.
%% Thus, if we take the size of a unit of composition to be the terms in a function, then $|X'| \approx |X| + |Y|$.
If we take the size of a unit of composition to be the number of terms in the definition of a function then $|X'| \approx |X| + |Y|$.
\end{enumerate}
A model adhering to these principles facilitates and supports \emph{principled composition and decomposition}.
Principled composition requires language support for interfaces, definitions, and substitutional equivalence.
A variety of useful domains including mathematical expressions, object-oriented programming, functional programming, and digital logic circuits support principled composition.

Asynchronous concurrency, an inherent characteristic of reactive systems, undermines decomposition and composition when not properly encapsulated and therefore limits our ability to design and implement complex reactive systems.
Such problems of asynchronous concurrency stem from the interactions between reactive programs.
Decomposition increases the number of reactive programs constituting a system which in turn increases the number of susceptible interactions and opportunities for timing bugs.
For decomposition to achieve an overall reduction in complexity when designing a reactive system, it must reduce the amount of reasoning that must be performed at each level of the design.
Thus, it must be possible to replace the details of how a reactive program is implemented with higher-level statements about its behavior in terms of its interface.

\paragraph{Limitations of the state of the art.}
State is fundamental to reactive systems and may be modeled directly or indirectly.
Some examples of indirect approaches to modeling state include monads~\cite{wadler1990comprehending} which are typically used in functional languages such as Haskell, or mail queues with message-behavior pairs~\cite{agha1985actors} which are used in actor-oriented languages such as Erlang.
The imperative programming paradigm models state directly using variables and assignment and the dominant languages used to implement reactive systems, such as C, C++, and Java, are based on this paradigm.
As we are interested in expressing reactive semantics directly, we limit the remaining discussion to reactive systems based on the imperative programming paradigm.

The imperative programming paradigm when implemented using structured programming techniques attempts to express a computation as a sequence of statements.
The computation being performed is realized by executing each statement in the sequence where a statement is either an assignment statement, a condition (if/else), or a loop.
Control flows from one statement to the next unless altered by a condition or loop.
A locus of control is called a \emph{thread}.
Statements compose sequentially, that is, a sequence of statements can be thought of as a single statement that performs the computation of the sequence in one step.
Imperative programming languages often permit the definition of procedures, e.g., subroutines, functions, macros, etc., as a way to abstract sequences of statements or expressions.
Control passes from the calling sequence to the sequence indicated by the procedure and returns to the calling sequence when the procedure terminates.
The semantics of calling a procedure are completely compatible with the sequential composition of statements.

Many reactive programs are multi-threaded and designed to use the facilities of a conventional operating system.
For the discussion to follow, we consider each thread in a multi-threaded program to be a reactive program.
The environment for a thread then is the operating system and the other threads with which it directly interacts.
This matches the definition of a reactive program as the thread will have ongoing interactions with its environment, i.e., interact with the operating system and other threads.
This also matches the definition of asynchronous concurrency as the thread and its environment share no common clock and can interact asynchronously.
To be more specific, a system call is asynchronous from the perspective of the operating system and threads may receive asynchronous signals from the operating system and each other.

Introducing asynchronous concurrency into the imperative programming paradigm places a burden on developers as they must explicitly identify the statement sequences that must atomic.
The misuse of synchronization primitives, which becomes likely as state transitions become complex, may introduce concurrency hazards (e.g., deadlock~\cite{dijkstra1965cooperating}) which manifest themselves as timing bugs.
A number of design patterns for concurrency have been developed to help developers avoid concurrency hazards~\cite{schmidt2000pattern}, \cite{lea2000concurrent}.
These design patterns represent a move toward implicit atomicity as they often attempt to leverage language features to control atomicity (e.g., scoped locking~\cite{schmidt2000pattern}).
While some of these patterns have been incorporated into programming languages (e.g., the \verb+synchronized+ keyword of Java implements the thread-safe interface pattern~\cite{schmidt2000pattern}), they are most often enforced only by convention and therefore easily violated or ignored (e.g., if a new developer is unaware of the convention).
In practice, explicit atomicity and the corresponding use of synchronization primitives has proven to be tedious and error prone~\cite{sutter2005software}.

Correct synchronization in sequential imperative programs is a holistic problem that resists encapsulation.
Sequential imperative programs, both transformational and reactive, are often designed using functionally modular principles such as procedural programming, object-oriented programming, and functional programming.
A transition in a reactive program then is typically distributed over a variety of modules, i.e., the graph of procedure calls.
The challenge for a developer then is to identify all of the shared state and then use synchronization primitives to guard concurrent updates.
Proper synchronization is based upon a complete understanding of the call graph (which may not be fully known due to aliasing).
The resulting code tends to be brittle as modifications tend to introduce new timing bugs.
%% Existing approaches to synchronization attempt to impose structure on the call graph, e.g., an object can only participate in one transition at a time (thread-safe interface~\cite{schmidt2000pattern}), to avoid concurrency hazards.
%% The most common formal approach to verifying correct synchronization is model checking which has its own limitation, e.g., the size and fidelity of the model.
%% The active object pattern~\cite{schmidt2000pattern} is an attempt to encapsulate the behavior of a reactive program.

Reactive programs based on the imperative programming paradigm are not subject to principled decomposition and composition.
To illustrate, consider three imperative reactive programs $A$, $X$, and $Y$ where $A$ is composed of $X$ and $Y$.
Principled composition requires that the definition of $A$ can be formed by substituting the definitions of $X$ and $Y$.
To preserve the reactive semantics when composing $X$ and $Y$, one must consider all pairs of transitions, i.e., the Cartesian product, which represents all possible interleavings between the two statement sequences.
The result of composition then is a two-dimensional torus where each node represents a compound state, each edge represents a transition, and each direction corresponds to executing a statement in $X$ and/or $Y$.
Appropriate measures must be taken to ensure that all transitions, i.e., all vertical, horizontal, and diagonal moves in the torus, are well-defined, i.e., explicit atomicity.
We observe that 1)~no existing platforms support the direct definition of such tori and 2)~reasoning about a two-dimensional torus (or $N$-dimensional for $N$ composed sequences) is qualitatively different that reasoning about a single sequence.
Thus, reactive programs based on the imperative programming paradigm are not subject to recursive encapsulation and substitutional equivalence.


%% Responsiveness or the ability to consume input promptly is a goal for many reactive systems.
%% One approach is for the producer to asynchronously signal a consumer when input is available.
%% In user space, this customarily referred to as a \emph{signal} and in kernel space, this is customarily referred to as an \emph{interrupt}.
%% The receipt of a signal diverts the flow of control to a \emph{signal handler} which typically restores the ``normal'' flow of control upon completion.
%% The hazard of asynchronous input when using deterministic sequencing is that the signal handler may update the state of the system in arbitrary ways.
%% Thus, state updated in asynchronous contexts must be protected against concurent updates, e.g., by disabling interrupts in operating systems~\cite{corbet2005linux}.
%% In user space, synchronous I/O multiplexing, e.g., the \verb+select+ system call, is often preferred to (asynchronous) signals precisely because of this disruption in the ``normal'' flow of control.
%% While asynchronous input is avoidable in user space, asynchronous input, i.e., interrupts, is necessary in kernel space as software interrupts are often the basis for system calls including synchronous output and hardware interrupts are often the basis for interacting with physical devices.

%% The activities of input, output, and internal processing correspond to reading shared state, writing shared state, and updating private state.
%% A number of modes for input and output have developed in platforms based on sequential composition for complex state transitions.
%% A \emph{synchronous} operation is one that occurs within the normal flow of control.
%% An \emph{asynchronous} operation disrupts the normal flow of control but typically resumes the normal flow of control after the disruption.
%% Input may either be synchronous or asynchronous but output is always synchronous as it is under the control of the reactive process.
%% A \emph{blocking} operation suspends the normal flow of control until the operation may be completed.
%% A \emph{non-blocking} operation executes if possible without suspending the normal flow of control.
%% Non-blocking operations must be used if a reactive process needs to be responsive to multiple ``clients.''

%% Practice shows that asynchronous input is tedious and error prone when compared to synchronous input.
%% The hazard of asynchronous input is that code executed by the diverted flow may update the state of the system in arbitrary ways.
%% Data structures used in asynchronous contexts must be guarded, e.g., by disabling interrupts in operating systems~\cite{corbet2005linux}.
%% In user space, synchronous I/O multiplexing, e.g., the \verb+select+ system call, is often preferred to (asynchronous) signals precisely because of the disruption in the ``normal'' flow of control.
%% While asynchronous input is avoidable in user space, asynchronous input, i.e., interrupts, is necessary in kernel space as software interrupts are often the basis for system calls including synchronous output and hardware interrupts are often the basis for interacting with physical devices.

%% Don't share variables, share transitions!!

%% The activities of input, output, and internal processing correspond to reading shared state, writing shared state, and updating private state.
%% A number of modes for input and output have developed in platforms based on sequential composition for complex state transitions.
%% A \emph{synchronous} operation is one that occurs within the normal flow of control.
%% An \emph{asynchronous} operation disrupts the normal flow of control but typically resumes the normal flow of control after the disruption.
%% Input may either be synchronous or asynchronous but output is always synchronous as it is under the control of the reactive process.
%% A \emph{blocking} operation suspends the normal flow of control until the operation may be completed.
%% A \emph{non-blocking} operation executes if possible without suspending the normal flow of control.
%% Non-blocking operations must be used if a reactive process needs to be responsive to multiple ``clients.''

%% Practice shows that asynchronous input is tedious and error prone when compared to synchronous input.
%% The hazard of asynchronous input is that code executed by the diverted flow may update the state of the system in arbitrary ways.
%% Data structures used in asynchronous contexts must be guarded, e.g., by disabling interrupts in operating systems~\cite{corbet2005linux}.
%% In user space, synchronous I/O multiplexing, e.g., the \verb+select+ system call, is often preferred to (asynchronous) signals precisely because of the disruption in the ``normal'' flow of control.
%% While asynchronous input is avoidable in user space, asynchronous input, i.e., interrupts, is necessary in kernel space as software interrupts are often the basis for system calls including synchronous output and hardware interrupts are often the basis for interacting with physical devices.

%% For complex reactive systems, we are interested in the effect of sequential composition for complex state transitions on the composition of reactive programs.
%% Reactive systems implemented using sequential composition typically use threads and/or events and modular principles such as procedures and object-oriented programming.
%% In general, a system designed and implemented exclusively with threads is organized into a number of objects and threads that represent concurrent activities.
%% In a web server, for example, each thread might process a request while various objects provide the data used to generate the response.
%% After a thread is created or dispatched in response to a new input, the thread then invokes a number of procedures to accomplish its work while possibly producing output events before exiting or returning to a pool to be dispatched again.

%% STRIKE REACTIVE BEHAVIOR

%% With respect to composition and decomposition, reactive behavior is not encapsulated in designs using threads.
%% %% In the examples above, module boundaries roughly correspond to objects.
%% A thread that enters an object (i.e., invokes a method on it) can also enter any other object whose identity is known to that object.
%% Thus, the behavior of a thread may be distributed over a variety of modules.
%% Proper synchronization, then, is based upon a complete understanding of the call graph (which may not be fully known due to aliasing).
%% Existing approaches to synchronization attempt to impose structure on the call graph, e.g., only one object is active at a time (thread-safe interface~\cite{schmidt2000pattern}), to avoid concurrency hazards.
%% %% Existing approaches are often expressed as design patterns, meaning that they are enforced by convention and are easily violated or ignored (e.g., a new developer is unaware of the convention).
%% The difficulties of using multiple threads are well known~\cite{lee2006problem}.

%% Events have been proposed as an alternative to threads~\cite{ousterhout1996threads}.
%% A system designed and implemented exclusively with events consists of a single flow of control that invokes an event handler upon reception of an input.
%% Since event handlers run to completion, event handlers should not invoke any blocking operations but should instead use non-blocking or asynchronous I/O to ensure responsiveness.
%% Events can be seen as an attempt to realize implicit atomicity since the absence of multiple flows of control guarantees that each event is atomic.
%% A computation normally performed by one thread is distributed over a set of event handlers that are punctuated by I/O operations.
%% The context of each computation then must be managed explicitly which is referred to as ``stack ripping\cite{adya2002cooperative}.''
%% Event handlers from different logical computations may be interleaved giving the illusion of concurrency while avoiding the challenges of synchronization.
%% Event systems wishing to take advantage of true concurrency must use multiple event loops and face all of the challenges of multi-threaded programming.

%% Unfortunately, reactive behavior is not encapsulated in designs using events, either.
%% While there is no danger of corruption due to multiple threads, there is the possibility that a thread will enter the same object twice due to a cycle in the graph of objects.
%% Thus, the caveats pertaining to call graphs in threaded designs also apply to event-based designs.
%% Furthermore, the set of available events in a system is limited to those defined by the module running the loop that dispatches events.

%% \paragraph{Requirements.}
%% A reduction in the accidental complexity that accompanies reactive systems and the broad-scale application of composition and decomposition to the design and implementation of reactive systems requires a model and platform for reactive programs with a combination of features not found in existing models and platforms.
%% To achieve correct semantics in the face of asynchrony and concurrency, while allowing useful decomposition and composition, the following requirements must be met.

%% First, atomicity must be implicit.
%% This implies that parallel composition is the default means of constructing complex state transitions.
%% Decomposing a complex atomic transition into a sequence is typically an \emph{optimization}, e.g., increasing parallelism, improving responsiveness by making critical sections smaller, etc. and should be treated as such.
%% Consequently, the sequencing required to decompose a complex state transition into a sequence of transitions must be made explicit by the developer.
%% Expressions may use sequential composition with the understanding that the sequence is part of an atomic operation.

%% Second, reactive behaviors should be rigorously encapsulated in units of composition.
%% Such units of composition can be contrasted with threads whose behavior may be spread over a variety of modules.
%% Allowing reactive behavior to leak across module boundaries creates the potential for unintended interactions, e.g., race conditions, and undermines composition and decomposition.

%% Third, a model for reactive programs should support the recursive encapsulation of units of composition.
%% The result of decomposition is often a hierarchy where one system logically contains other systems.
%% Recursive encapsulation provides the flexibility needed to design and construct complex systems by allowing a designer to apply decomposition an arbitrary number of times.
%% Recursive encapsulation also promotes uniformity as the same reactive semantics are used to construct each module in a system.

%% Fourth, composition can only restrict the behavior of a unit of composition.
%% When introduced into a compositional context, the behavior exhibited by a unit of composition within that context must be a subset of the overall set of possible behaviors of the unit of composition.
%% This can be contrasted readily with threads where the behavior of a thread can easily be altered by introducing a new thread.
%% The concept of a unit of composition with a closed set of behaviors is thus inherent to reusable reactive software.

%% Fifth, units of composition should be compositional meaning that the properties of a unit of composition can be expressed in terms of the properties of its constituent members.
%% Thus, the behavior of a unit of composition can be understood by understanding the behavior and interactions of its members.

%% Sixth, a model for reactive programs requires implementation support.
%% One alternative is to add concurrency and asynchrony to an existing language via a library, e.g., POSIX threads in C, which changes the semantics of the language without changing its syntax~\cite{lee2006problem}.
%% However, changing semantics without changing syntax may be detrimental to programmers as they are forced to go outside of the language when reasoning about a system.
%% Also, changing semantics without changing syntax forgoes the opportunity for a compiler to check the application of reactive semantics for correctness.
%% In order to build the complex reactive systems we envision, efficient automated methods for analyzing reactive semantics must become part of the development work flow.



%% Sixth, a model for reactive programs must have an effective implementation.
%% Said another way, the model should not rely on techniques that cannot be implemented efficiently.

%% \paragraph{Solution approach.}
%% Actor models are currently being explored as an alternative to thread and event models.
%% The motivation behind actor models is the observation that most of the challenges associated with reactive programs are derived from shared (mutable) state and/or arbitrary control flows.
%% An actor has private state and (conceptually) a private persistent thread of control that processes inputs arriving on input ports and places outputs on ports that can be accessed by other actors~\cite{lee2011heterogeneous}.
%% Each actor model prescribes different semantics for the execution of the actors and ports.
%% In the Kahn process network model, for example, ports are infinite buffers with blocking reads and non-blocking writes~\cite{kahn1974semantics}.

%% The research proposed here can be thought of as providing a new actor model with semantics that meet the above requirements for the broad-scale application of composition and decomposition to reactive systems.
%% Instead of a sequential thread of control, an actor in our model comprises a set of atomic actions that may be scheduled non-deterministically.
%% When defined this way, a set of actors may be composed by taking the union of their programs, as opposed to taking their cross-product as would be the case with threads.
%% A port links actions in different actors and optionally transfers values from one actor to another.
%% Since ports link atomic actions in independent actors, one can relate the state and behavior of interacting actors and therefore make strong claims about the state and behavior of a network of actors.
%% Furthermore, designs in our model are not limited exclusively to pair-wise interactions since ports facilitate the construction of interactions that involve more than two parties.
%% The ports constitute an actor's interface and simultaneously encapsulate reactive behavior while providing a principled and flexible mechanism for composing reactive behaviors.

\paragraph{Approach.}
To reduce the accidental complexity associated with the design and implementation of reactive systems while supporting principled composition and decomposition, we propose a transactional model for reactive systems called reactive components.
A reactive component is a set of state variables and a set of atomic transitions that operate on those state variables.
Linking a transition in one component to a transition in another component yields another transition that operates on the state variables of the constituent components.
A transaction-oriented approach resolves the main difficulties of the control-oriented imperative approach.
In the reactive component model, transitions are atomic.
This relieves developers from the burden of identifying and guarding critical sections.
The composability of transitions allows developers to create complex state transitions independent of control flow.
This relieves developers from needing a holistic understanding of the call graph when composing a complex state transition.

\paragraph{Contributions.}
This research makes the following contributions to the state of the art in reactive system development.
In Chapter~\ref{model}, we present the reactive component model.
In Chapter~\ref{language}, we present \rcgo{}:  a programming language for reactive components based on the Go programming language.
For tractability, we assume systems with a fixed number of components in a fixed configuration.
In Chapter~\ref{implementation}, we describe the implementation of an interpreter for \rcgo{} including the algorithm that checks a system for sound composition, a calling convention for transitions, the implementation of move semantics, and an approach to file descriptor I/O.
Chapter~\ref{implementation} also describes the design, implementation, and an evaluation of two concurrent weakly fair schedulers.
The schedulers are compared to a custom multi-threaded application for two reactive systems.
For one system, the reactive component implementation outperforms the custom application while the custom application outperforms the reactive component implementation for the other system.
The results demonstrate that reactive components may be a viable alternative to threads but additional work is necessary to generalize this claim.
